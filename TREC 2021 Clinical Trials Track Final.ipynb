{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREC 2021 Clinical Trials Track Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the this task has the following main steps:\n",
    "- create an index mapping for the Clinical Trials Track documents\n",
    "\n",
    "\n",
    "- index the documents from the Clinical Trials Track into this index (use only the documents with qrel)\n",
    "\n",
    "\n",
    "- index the 75 queries into a different index, but with the same mapping structure\n",
    "\n",
    "\n",
    "- for each of the 75 queries, extract the top 10k documents for that query and copy all of the documents into a new index. This index is connected to the query and has the name format: \"10k_query_\" + str(i), where i is the query number/position\n",
    "\n",
    "\n",
    "- after each batch of 10k documents has been copied to it's respective query index, compute the df value, for each token in the queries. This is accomplished by using get_df_ttf(). These values will be later used for the computation of the BM25 variants.\n",
    "\n",
    "\n",
    "- after obtaining the df values, the next values to be determined are the tf values. Tf is the term frequency of term t, in document d. This means that in order to get the tf values from the ElasticSearch API, we need to query each document individually. To do so, iteratively:\n",
    "       1) we take each document from the 10k collection\n",
    "       2) we create a 'disposable' index into which we index the document\n",
    "       3) we query the 'disposable' index and get the tf values\n",
    "       4) we delete the 'disposable' index\n",
    "  \n",
    "  \n",
    "- because the above step takes a lot of time (approx. 3 hours), the process is repeated for one query. However, the entire pipeline can be repeated for each query in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "es = Elasticsearch(HOST=\"http://localhost\", PORT=9200, timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the index mapping\n",
    "index_body = {\n",
    "    \"settings\":{\n",
    "        \"index\":{\n",
    "        \"number_of_shards\":1,\n",
    "        \"number_of_replicas\":1\n",
    "        }\n",
    "    },\n",
    "    \"mappings\":{\n",
    "        \"properties\":{\n",
    "            \"Content\":{\n",
    "                \"type\":\"text\",\n",
    "                \"fielddata\":True,\n",
    "                \"term_vector\": \"with_positions_offsets_payloads\",\n",
    "                \"store\" : True,\n",
    "                \"analyzer\" : \"whitespace\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es.indices.create(index = \"clinical_trials_track\", body = index_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.exists(index = \"clinical_trials_track\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Clinical Trials dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "\n",
    "# load the dataset\n",
    "dataset = ir_datasets.load(\"clinicaltrials/2021/trec-ct-2021\")\n",
    "\n",
    "# get the doc_id of the documents that have qrel\n",
    "docs_with_qrel = []\n",
    "for doc in dataset.qrels_iter():\n",
    "    docs_with_qrel.append(doc.doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35832"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of documents that have qrel\n",
    "dataset.qrels_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35832/35832 [46:22<00:00, 12.88it/s]  \n"
     ]
    }
   ],
   "source": [
    "# create a docstore to easily retrieve the date based on \"doc_id\"\n",
    "docstore = dataset.docs_store()\n",
    "\n",
    "# iterate over the docs having qrel\n",
    "for docID in tqdm(docs_with_qrel):\n",
    "    # retrieve the document\n",
    "    doc = docstore.get(docID)\n",
    "    \n",
    "    # format the document\n",
    "    doc_formatted = {\n",
    "        'Content': doc.detailed_description,\n",
    "    }\n",
    "    \n",
    "    # index it using elastic search\n",
    "    es.index(index = \"clinical_trials_track\", id=docID, body=doc_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the querries\n",
    "import csv\n",
    "\n",
    "queries = []\n",
    "\n",
    "with open(\"./queries_2021.tsv\", encoding=\"utf8\") as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\n\")\n",
    "     \n",
    "    for q in tsv_file:\n",
    "        queries.append(q)\n",
    "        \n",
    "queries = [q[0] for q in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the querries \n",
    "formatted_queries = []\n",
    "\n",
    "for q in queries:\n",
    "    q = q.replace(\"\\t\", \" \")\n",
    "    formatted_queries.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'query_index'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new index for queries\n",
    "es.indices.create(index = \"query_index\", body = index_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, query_text in enumerate(formatted_queries):\n",
    "    # format the document\n",
    "    query_dict = {\n",
    "        'Content': query_text,\n",
    "    }\n",
    "    \n",
    "    query_id = 'query' + str(i) \n",
    "    \n",
    "    # index the query using elastic search\n",
    "    es.index(index = \"query_index\", id=query_id, body=query_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a query, get the top 10k documents and move them to a separate index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'clinical_trials_track_clone'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a clone of the original index\n",
    "es.indices.create(index = \"clinical_trials_track_clone\", body = index_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.exists(index = \"clinical_trials_track_clone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 11269,\n",
       " 'timed_out': False,\n",
       " 'total': 26162,\n",
       " 'updated': 26162,\n",
       " 'created': 0,\n",
       " 'deleted': 0,\n",
       " 'batches': 27,\n",
       " 'version_conflicts': 0,\n",
       " 'noops': 0,\n",
       " 'retries': {'bulk': 0, 'search': 0},\n",
       " 'throttled_millis': 0,\n",
       " 'requests_per_second': -1.0,\n",
       " 'throttled_until_millis': 0,\n",
       " 'failures': []}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reindex_body = {\n",
    "    \"source\": {\n",
    "        \"index\": \"clinical_trials_track\",\n",
    "    },\n",
    "    \"dest\": {\n",
    "        \"index\": \"clinical_trials_track_clone\"\n",
    "    }\n",
    "}\n",
    "\n",
    "es.reindex(body = reindex_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in clinical_trials_track index: 26162\n",
      "Elements in clinical_trials_track_clone index: 26162\n"
     ]
    }
   ],
   "source": [
    "# check to see if all elementes have been reindexed\n",
    "k1 = es.count(index = \"clinical_trials_track\")['count']\n",
    "k2 = es.count(index = \"clinical_trials_track_clone\")['count']\n",
    "\n",
    "print(f\"Elements in clinical_trials_track index: {k1}\")\n",
    "print(f\"Elements in clinical_trials_track_clone index: {k2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(formatted_queries)):\n",
    "    new_index = '10k_query_' + str(i)\n",
    "    es.indices.delete(index = new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [09:25,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, query_text in tqdm(enumerate(formatted_queries)):\n",
    "    new_index = '10k_query_' + str(i)\n",
    "    \n",
    "    # create the index\n",
    "    es.indices.create(index = new_index, body = index_body)\n",
    "    \n",
    "    # extract the top 10k documents and reindex them into the new index\n",
    "    reindex_body = {\n",
    "        \"max_docs\": 10000,\n",
    "        \"source\": {\n",
    "            \"index\": \"clinical_trials_track_clone\",\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"should\": [\n",
    "                        {\"match\": \n",
    "                             {\"Content\": query_text}\n",
    "                        }\n",
    "                    ] \n",
    "                ,\"minimum_should_match\": 1,\n",
    "                \"boost\": 1.0\n",
    "                }\n",
    "            }\n",
    "\n",
    "        },\n",
    "        \"dest\": {\n",
    "            \"index\": new_index\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    es.reindex(body = reindex_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abovee cell has created, for each of the 75 querries, an index containing the top 10k documents retrieved from the main corpus for that specific query.\n",
    "\n",
    "The name of the indexes is formatted in the following way: '10k_query_' + str(i), where i is the position of the query in the list formatted_queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute BM25 Variants\n",
    "\n",
    "DEFINITIONS:\n",
    "\n",
    "- N is the number of documents in the collection\n",
    "- df_t is the number of documents containing term t\n",
    "- tf_td is the term frequency of term t in document d\n",
    "- L_d is the number of tokens in document d\n",
    "- L_avg is the average number of tokens in a document in the collection\n",
    "- k1, b are free parameters that can be optimized per collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the original data of the documents returned by the es.search() method\n",
    "def get_original_data(query_position):\n",
    "    bool_query = {\n",
    "        \"size\": 10000,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\"match\": \n",
    "                         {\"Content\": formatted_queries[query_position]}\n",
    "                    }\n",
    "                ] \n",
    "            ,\"minimum_should_match\": 1,\n",
    "            \"boost\": 1.0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    index_name = '10k_query_' + str(query_position)\n",
    "    \n",
    "    search_returns = es.search(index = '10k_query_1',  body = bool_query)\n",
    "    \n",
    "    doc_ids_list = []\n",
    "    doc_original_scores = []\n",
    "    doc_contents = []\n",
    "    \n",
    "    for j in range(len(search_returns['hits']['hits'])):\n",
    "        # save the id, original score and content of the to 10k documents\n",
    "        current_id = search_returns['hits']['hits'][j]['_id']\n",
    "        doc_ids_list.append(current_id)\n",
    "        \n",
    "        current_score = search_returns['hits']['hits'][j]['_score']\n",
    "        doc_original_scores.append(current_score)\n",
    "        \n",
    "        current_doc_content = search_returns['hits']['hits'][j]['_source']\n",
    "        doc_contents.append(current_doc_content)\n",
    "        \n",
    "    return [doc_ids_list, doc_original_scores, doc_contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of tokens per document (i.e. per query)\n",
    "def compute_L_d(index_name, document_ID):\n",
    "    res_dict = es.termvectors(index = index_name, id = document_ID)\n",
    "    \n",
    "    if 'term_vectors' in res_dict:\n",
    "        L_d = len(res_dict['term_vectors']['Content']['terms'].keys())\n",
    "    \n",
    "    else:\n",
    "        L_d = 0\n",
    "    \n",
    "    return L_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_L_avg(index_name, doc_ids_list):\n",
    "    L_total = 0\n",
    "    for i in range(len(doc_ids_list)):\n",
    "        individual_term_vectors_dict = es.termvectors(index = index_name, id = doc_ids_list[i])\n",
    "        \n",
    "        current_length = len(dd['term_vectors']['Content']['terms'].keys())\n",
    "        \n",
    "        L_total += current_length\n",
    "        \n",
    "    L_avg = L_total/len(doc_ids_list)\n",
    "    \n",
    "    return L_avg        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of documents in the collection is always 10k\n",
    "N = 10000\n",
    "\n",
    "# use pre-set values for k1 and b\n",
    "k1 = 0.9\n",
    "b = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ATIRE and Lucene score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_ttf(query_text, index_name):\n",
    "    \n",
    "    # format the doc body for the es.termvectors() method\n",
    "    doc = {\n",
    "      \"term_statistics\" : True,\n",
    "      \"doc\" : {\n",
    "        \"Content\" : query_text\n",
    "      }\n",
    "    }\n",
    "\n",
    "    result = es.termvectors(index = index_name, body = doc)\n",
    "    \n",
    "    terms_dic = result['term_vectors']['Content']['terms']\n",
    "    \n",
    "    total_df = result['term_vectors']['Content']['field_statistics']['sum_doc_freq']\n",
    "    total_ttf = result['term_vectors']['Content']['field_statistics']['sum_ttf']\n",
    "    # create a dictionary in which to save the df-value per term/token of query\n",
    "    df_per_query_term = {}\n",
    "    ttf_per_query_term = {}\n",
    "    \n",
    "    for term in terms_dic.keys():\n",
    "        if 'doc_freq' in terms_dic[term]:\n",
    "            df = terms_dic[term]['doc_freq'] / total_df\n",
    "            ttf = terms_dic[term]['ttf'] / total_ttf\n",
    "            \n",
    "            df_per_query_term[str(term)] = df\n",
    "            ttf_per_query_term[str(term)] = ttf\n",
    "            \n",
    "    return [df_per_query_term, ttf_per_query_term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ATIRE score for each query and store them in a pandas dataframe\n",
    "results_per_query = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,2):\n",
    "    print(f'Starting computations for query: {i}')\n",
    "    query_text = formatted_queries[i]\n",
    "    index_name = '10k_query_' + str(i)\n",
    "    \n",
    "    b = 0.4\n",
    "    k1 = 0.9\n",
    "    N = 10000\n",
    "    \n",
    "    # ======== FIRST GET THE DATA THAT APPLIES TO THE ENTIRE CORPUS ========\n",
    "    # get the original data from the es.search() method\n",
    "    original_data = get_original_data(i)\n",
    "    \n",
    "    doc_ids_list = original_data[0]\n",
    "    doc_original_scores = original_data[1]\n",
    "    doc_original_contents = original_data[2]\n",
    "    \n",
    "    df_per_query = pd.DataFrame()\n",
    "    df_per_query['doc_ids'] = doc_ids_list\n",
    "    df_per_query['original_scores'] = doc_original_scores\n",
    "    \n",
    "    # Compute the average length of the documents:\n",
    "    L_avg = compute_L_avg(index_name, doc_ids_list)\n",
    "    \n",
    "    # get the df-values for each term of the query (note: df values apply for the entire corpus of documents)\n",
    "    [df_per_query_term, ttf_per_query_term] = get_df_ttf(query_text, index_name)\n",
    "    \n",
    "    # format the doc body for the es.termvectors() method\n",
    "    doc_body = {\n",
    "      \"term_statistics\" : True,\n",
    "      \"doc\" : {\n",
    "        \"Content\" : query_text\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    # ======== ======== ======== ======== ======== ======== ======== ========\n",
    "    \n",
    "    # ======== NEXT GET THE DATA THAT IS COMPUTED PER DOCUMENT ========\n",
    "   \n",
    "    \n",
    "    # for each document in the ranking of the query, re-compute the scores\n",
    "    ATIRE_scores = []\n",
    "    Lucene_accurate_scores = []\n",
    "    \n",
    "    for j in tqdm(range(len(doc_ids_list))):\n",
    "        ATIRE_score_per_doc = 0\n",
    "        Lucene_accurate_score_per_doc = 0\n",
    "        \n",
    "        docID = doc_ids_list[j]\n",
    "        doc_content = doc_original_contents[j]\n",
    "        \n",
    "        # compute L_d\n",
    "        L_d = compute_L_d(index_name, docID)\n",
    "        \n",
    "        # index the document content into a 'disposable index'\n",
    "        es.indices.delete(index = 'disposable')\n",
    "        es.indices.create(index = 'disposable', body = index_body)\n",
    "        es.index(index = 'disposable', body = doc_content)\n",
    "\n",
    "        disposable_result = es.termvectors(index = \"disposable\", body = doc_body)\n",
    "        disposable_dict = disposable_result['term_vectors']['Content']['terms']\n",
    "        \n",
    "        for term in disposable_dict.keys():\n",
    "            if term in df_per_query_term:\n",
    "                df = df_per_query_term[term]\n",
    "                ttf = ttf_per_query_term[term]\n",
    "                \n",
    "                tf = disposable_dict[term]['term_freq'] / ttf\n",
    "    \n",
    "                # Compute ATIRE score\n",
    "                x1_ATIRE = np.log(N/df)\n",
    "                x2_ATIRE = (tf*(k1+1)) / (tf + k1*(1 - b + b*(L_d/L_avg)))\n",
    "                \n",
    "                ATIRE_score_per_doc += x1_ATIRE * x2_ATIRE\n",
    "                \n",
    "                # Compute Lucene Accurate score\n",
    "                x1_lucene = np.log(1+ (N - df + 0.5)/(df + 0.5))\n",
    "                x2_lucene = tf / (tf + k1*(1 - b + b*(L_d/L_avg)))\n",
    "                Lucene_accurate_score_per_doc += x1_lucene * x2_lucene\n",
    "                \n",
    "                \n",
    "        \n",
    "        # save the score per document:\n",
    "        ATIRE_scores.append(ATIRE_score_per_doc) \n",
    "        Lucene_accurate_scores.append(Lucene_accurate_score_per_doc)\n",
    "#         print(f\"ATIRE_score_per_doc: {ATIRE_score_per_doc}, Lucene_accurate_score_per_doc: {Lucene_accurate_score_per_doc}\")\n",
    "\n",
    "#         es.indices.delete(index = 'disposable')\n",
    "        \n",
    "    df_per_query['ATIRE_scores'] = ATIRE_scores\n",
    "    df_per_query['Lucene_accurate_scores'] = Lucene_accurate_scores\n",
    "    \n",
    "    query_ID = 'query' + str(i)\n",
    "    results_per_query[query_ID] = df_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query1 = results_per_query['query1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query1.to_csv('./query1_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting computations for query: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [2:51:48<00:00,  1.03s/it]  \n"
     ]
    }
   ],
   "source": [
    "for i in range(2,3):\n",
    "    print(f'Starting computations for query: {i}')\n",
    "    query_text = formatted_queries[i]\n",
    "    index_name = '10k_query_' + str(i)\n",
    "    \n",
    "    b = 0.4\n",
    "    k1 = 0.9\n",
    "    N = 10000\n",
    "    \n",
    "    # ======== FIRST GET THE DATA THAT APPLIES TO THE ENTIRE CORPUS ========\n",
    "    # get the original data from the es.search() method\n",
    "    original_data = get_original_data(i)\n",
    "    \n",
    "    doc_ids_list = original_data[0]\n",
    "    doc_original_scores = original_data[1]\n",
    "    doc_original_contents = original_data[2]\n",
    "    \n",
    "    df_per_query = pd.DataFrame()\n",
    "    df_per_query['doc_ids'] = doc_ids_list\n",
    "    df_per_query['original_scores'] = doc_original_scores\n",
    "    \n",
    "    # Compute the average length of the documents:\n",
    "    L_avg = compute_L_avg(index_name, doc_ids_list)\n",
    "    \n",
    "    # get the df-values for each term of the query (note: df values apply for the entire corpus of documents)\n",
    "    [df_per_query_term, ttf_per_query_term] = get_df_ttf(query_text, index_name)\n",
    "    \n",
    "    # format the doc body for the es.termvectors() method\n",
    "    doc_body = {\n",
    "      \"term_statistics\" : True,\n",
    "      \"doc\" : {\n",
    "        \"Content\" : query_text\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    # ======== ======== ======== ======== ======== ======== ======== ========\n",
    "    \n",
    "    # ======== NEXT GET THE DATA THAT IS COMPUTED PER DOCUMENT ========\n",
    "   \n",
    "    \n",
    "    # for each document in the ranking of the query, re-compute the scores\n",
    "    ATIRE_scores = []\n",
    "    Lucene_accurate_scores = []\n",
    "    \n",
    "    for j in tqdm(range(len(doc_ids_list))):\n",
    "        ATIRE_score_per_doc = 0\n",
    "        Lucene_accurate_score_per_doc = 0\n",
    "        \n",
    "        docID = doc_ids_list[j]\n",
    "        doc_content = doc_original_contents[j]\n",
    "        \n",
    "        # compute L_d\n",
    "        L_d = compute_L_d(index_name, docID)\n",
    "        \n",
    "        # index the document content into a 'disposable index'\n",
    "        es.indices.delete(index = 'disposable')\n",
    "        es.indices.create(index = 'disposable', body = index_body)\n",
    "        es.index(index = 'disposable', body = doc_content)\n",
    "\n",
    "        disposable_result = es.termvectors(index = \"disposable\", body = doc_body)\n",
    "        disposable_dict = disposable_result['term_vectors']['Content']['terms']\n",
    "        \n",
    "        for term in disposable_dict.keys():\n",
    "            if term in df_per_query_term:\n",
    "                df = df_per_query_term[term]\n",
    "                ttf = ttf_per_query_term[term]\n",
    "                \n",
    "                tf = disposable_dict[term]['term_freq'] / ttf\n",
    "    \n",
    "                # Compute ATIRE score\n",
    "                x1_ATIRE = np.log(N/df)\n",
    "                x2_ATIRE = (tf*(k1+1)) / (tf + k1*(1 - b + b*(L_d/L_avg)))\n",
    "                \n",
    "                ATIRE_score_per_doc += x1_ATIRE * x2_ATIRE\n",
    "                \n",
    "                # Compute Lucene Accurate score\n",
    "                x1_lucene = np.log(1+ (N - df + 0.5)/(df + 0.5))\n",
    "                x2_lucene = tf / (tf + k1*(1 - b + b*(L_d/L_avg)))\n",
    "                Lucene_accurate_score_per_doc += x1_lucene * x2_lucene\n",
    "                \n",
    "                \n",
    "        \n",
    "        # save the score per document:\n",
    "        ATIRE_scores.append(ATIRE_score_per_doc) \n",
    "        Lucene_accurate_scores.append(Lucene_accurate_score_per_doc)\n",
    "#         print(f\"ATIRE_score_per_doc: {ATIRE_score_per_doc}, Lucene_accurate_score_per_doc: {Lucene_accurate_score_per_doc}\")\n",
    "\n",
    "#         es.indices.delete(index = 'disposable')\n",
    "        \n",
    "    df_per_query['ATIRE_scores'] = ATIRE_scores\n",
    "    df_per_query['Lucene_accurate_scores'] = Lucene_accurate_scores\n",
    "    \n",
    "    query_ID = 'query' + str(i)\n",
    "    results_per_query[query_ID] = df_per_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query2 = results_per_query['query2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query2.to_csv('./query2_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
